{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SimCLR implementation #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation following: https://theaisummer.com/simclr/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import STL10, EuroSAT\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.multiprocessing import cpu_count\n",
    "from pytorch_lightning.callbacks import GradientAccumulationScheduler, ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "means = [87.81586935763889, 96.97416420717593, 103.98142336697049]\n",
    "stds = [51.67849701591506, 34.908630837585186, 29.465280593587384]\n",
    "\n",
    "def imshow(img, norm_means, norm_stds):\n",
    "    \"\"\"\n",
    "    shows an imagenet-normalized image on the screen\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(norm_means, dtype=torch.float32)\n",
    "    std = torch.tensor(norm_stds, dtype=torch.float32)\n",
    "    unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    npimg = unnormalize(img).numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def reproducibility(config):\n",
    "    SEED = int(config.seed)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "    if config.cuda:\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def device_as(t1, t2):\n",
    "    \"\"\"\n",
    "    Moves t1 to the device of t2\n",
    "    \"\"\"\n",
    "    return t1.to(t2.device)\n",
    "\n",
    "\n",
    "def define_parameter_groups(model, weight_decay, optimizer_name):\n",
    "    def exclude_from_weight_decay_and_adaptation(name):\n",
    "        if \"bn\" in name:\n",
    "            return True\n",
    "        if optimizer_name == \"lars\" and \"bias\" in name:\n",
    "            return True\n",
    "\n",
    "    param_groups = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if not exclude_from_weight_decay_and_adaptation(name)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"layer_adaptation\": True,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if exclude_from_weight_decay_and_adaptation(name)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"layer_adaptation\": False,\n",
    "        },\n",
    "    ]\n",
    "    return param_groups\n",
    "\n",
    "def default(val, def_val):\n",
    "    return def_val if val is None else val\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Augment:\n",
    "    \"\"\"\n",
    "    a probabilistic data augmentation module\n",
    "    Transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x_i and  x_j which we consider a positive pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, norm_means, norm_stds, s=1):\n",
    "        color_jitter = T.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "        # 10% of the image\n",
    "        blur = T.GaussianBlur(\n",
    "            kernel_size=(\n",
    "                3,\n",
    "                3,\n",
    "            ),\n",
    "            sigma=(0.1, 0.2),\n",
    "        )\n",
    "\n",
    "        self.train_transform = T.Compose(\n",
    "            [\n",
    "                # Crop image on a random scale from 7% tpo 100%\n",
    "                T.RandomResizedCrop(size=img_size),\n",
    "                # Flip image horizontally with 50% probability\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                # Apply heavy color jitter with 80% probability\n",
    "                T.RandomApply([color_jitter], p=0.8),\n",
    "                # Apply gaussian blur with 50% probability\n",
    "                T.RandomApply([blur], p=0.5),\n",
    "                # Convert RGB images to grayscale with 20% probability\n",
    "                T.RandomGrayscale(p=0.2),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(\n",
    "                    mean=norm_means,\n",
    "                    std=norm_stds,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AddProjection(nn.Module):\n",
    "    def __init__(self, config, model=None, mlp_dim=512):\n",
    "        super(AddProjection, self).__init__()\n",
    "        embedding_size = config.embedding_size\n",
    "        self.backbone = default(\n",
    "            model, models.resnet18(weights=None, num_classes=config.embedding_size)\n",
    "        )\n",
    "        mlp_dim = default(mlp_dim, self.backbone.fc.in_features)\n",
    "        print(\"DIM MLP input:\", mlp_dim)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # add mlp projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=mlp_dim, out_features=mlp_dim),\n",
    "            nn.BatchNorm1d(mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=mlp_dim, out_features=embedding_size),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        embedding = self.backbone(x)\n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        return self.projection(embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimCLRTraining(pl.LightningModule):\n",
    "    def __init__(self, config, norm_means, norm_stds, model=None, feat_dim=512):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.augment = Augment(config.img_size, norm_means=norm_means, norm_stds=norm_stds)\n",
    "        self.model = AddProjection(config, model=model, mlp_dim=feat_dim)\n",
    "\n",
    "        self.loss = InfoNceLoss(temperature=self.config.temperature)\n",
    "\n",
    "    def forward(self, batch, *args, **kwargs) -> torch.Tensor:\n",
    "        return self.model(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, *args, **kwargs) -> torch.Tensor:\n",
    "        (x1, x2), labels = batch\n",
    "        z1 = self.model(x1)\n",
    "        z2 = self.model(x2)\n",
    "        loss = self.loss(z1, z2)\n",
    "        self.log(\n",
    "            \"InfoNCE loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        wd = self.config.weight_decay\n",
    "        lr = self.config.lr\n",
    "        max_epochs = int(self.config.epochs)\n",
    "        param_groups = define_parameter_groups(\n",
    "            self.model, weight_decay=wd, optimizer_name=\"adam\"\n",
    "        )\n",
    "        optimizer = Adam(param_groups, lr=lr, weight_decay=wd)\n",
    "\n",
    "        print(\n",
    "            f\"Optimizer Adam, \"\n",
    "            f\"Learning Rate {lr}, \"\n",
    "            f\"Effective batch size {self.config.batch_size * self.config.gradient_accumulation_steps}\"\n",
    "        )\n",
    "\n",
    "        scheduler_warmup = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer, warmup_epochs=10, max_epochs=max_epochs, warmup_start_lr=0.0\n",
    "        )\n",
    "        return [optimizer], [scheduler_warmup]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_gpus: 0\n",
      "DIM MLP input: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderlontke/.conda/envs/ssl-remote-sensing/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/alexanderlontke/.conda/envs/ssl-remote-sensing/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /Users/alexanderlontke/Documents/Uni/St. Gallen/HS_22_23/deep_learning/ssl-remote-sensing/notebooks/saved_models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | AddProjection | 11.5 M\n",
      "1 | loss  | InfoNceLoss   | 0     \n",
      "----------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.024    Total estimated model params size (MB)\n",
      "/Users/alexanderlontke/.conda/envs/ssl-remote-sensing/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Adam, Learning Rate 0.001, Effective batch size 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46d72a726262411dbf80ea37b8097097"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask torch.Size([128, 128])\n",
      "exp torch.Size([128, 128])\n",
      "mask torch.Size([128, 128])\n",
      "exp torch.Size([128, 128])\n",
      "mask torch.Size([128, 128])\n",
      "exp torch.Size([128, 128])\n",
      "mask torch.Size([16, 16])\n",
      "exp torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# Machine setup\n",
    "available_gpus = torch.cuda.device_count()\n",
    "save_model_path = os.path.join(os.getcwd(), \"saved_models/\")\n",
    "print(\"available_gpus:\", available_gpus)\n",
    "\n",
    "# Run setup\n",
    "filename = \"SimCLR_ResNet18_adam\"\n",
    "save_name = filename + '.ckpt'\n",
    "resume_from_checkpoint = False\n",
    "\n",
    "\n",
    "# Model Setup\n",
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        self.epochs = 1  # number of training epochs\n",
    "        self.seed = 1234  # randomness seed\n",
    "        self.cuda = False  # use nvidia gpu\n",
    "        self.img_size = 64  #image shape\n",
    "        self.save = \"./saved_models/\"  # save checkpoint\n",
    "        self.gradient_accumulation_steps = 1  # gradient accumulation steps\n",
    "        self.batch_size = 64\n",
    "        self.lr = 1e-3\n",
    "        self.embedding_size = 128  # papers value is 128\n",
    "        self.temperature = 0.5  # 0.1 or 0.5\n",
    "        self.weight_decay = 1e-6\n",
    "\n",
    "\n",
    "train_config = Hparams()\n",
    "reproducibility(train_config)\n",
    "\n",
    "model = SimCLRTraining(\n",
    "    config=train_config,\n",
    "    model=models.resnet18(weights=None),\n",
    "    feat_dim=512,\n",
    "    norm_means=means,\n",
    "    norm_stds=stds\n",
    ")\n",
    "\n",
    "transform = Augment(\n",
    "    train_config.img_size, norm_means=means, norm_stds=stds\n",
    ")\n",
    "\n",
    "dataset = EuroSAT(\n",
    "    \"/Users/alexanderlontke/datasets/\",\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    dataset=torch.utils.data.Subset(dataset,range(200)),\n",
    "    batch_size=train_config.batch_size,\n",
    "    num_workers=cpu_count()\n",
    ")\n",
    "\n",
    "# Needed to get simulate a large batch size\n",
    "accumulator = GradientAccumulationScheduler(scheduling={0: 1})\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename=filename,\n",
    "    dirpath=save_model_path,\n",
    "    every_n_epochs=2,\n",
    "    save_last=True,\n",
    "    save_top_k=2,\n",
    "    monitor='Contrastive loss_epoch',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    trainer = Trainer(\n",
    "        callbacks=[accumulator, checkpoint_callback],\n",
    "        gpus=available_gpus,\n",
    "        max_epochs=train_config.epochs,\n",
    "        resume_from_checkpoint=train_config.checkpoint_path\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        callbacks=[accumulator, checkpoint_callback],\n",
    "        gpus=available_gpus,\n",
    "        max_epochs=train_config.epochs\n",
    "    )\n",
    "\n",
    "trainer.fit(model, data_loader)\n",
    "trainer.save_checkpoint(save_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ssl-remote-sensing",
   "language": "python",
   "display_name": "ssl-remote-sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}