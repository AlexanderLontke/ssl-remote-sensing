{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SimCLR implementation #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation following: https://theaisummer.com/simclr/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch torchvision pytorch-lightning lightning-bolts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import STL10, EuroSAT\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.multiprocessing import cpu_count\n",
    "from pytorch_lightning.callbacks import GradientAccumulationScheduler, ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means = [87.81586935763889, 96.97416420717593, 103.98142336697049]\n",
    "stds = [51.67849701591506, 34.908630837585186, 29.465280593587384]\n",
    "\n",
    "\n",
    "def imshow(img, norm_means, norm_stds):\n",
    "    \"\"\"\n",
    "    shows an imagenet-normalized image on the screen\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(norm_means, dtype=torch.float32)\n",
    "    std = torch.tensor(norm_stds, dtype=torch.float32)\n",
    "    unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    npimg = unnormalize(img).numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def reproducibility(config):\n",
    "    SEED = int(config.seed)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "    if config.cuda:\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def device_as(t1, t2):\n",
    "    \"\"\"\n",
    "    Moves t1 to the device of t2\n",
    "    \"\"\"\n",
    "    return t1.to(t2.device)\n",
    "\n",
    "\n",
    "def define_parameter_groups(model, weight_decay, optimizer_name):\n",
    "    def exclude_from_weight_decay_and_adaptation(name):\n",
    "        if \"bn\" in name:\n",
    "            return True\n",
    "        if optimizer_name == \"lars\" and \"bias\" in name:\n",
    "            return True\n",
    "\n",
    "    param_groups = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if not exclude_from_weight_decay_and_adaptation(name)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"layer_adaptation\": True,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if exclude_from_weight_decay_and_adaptation(name)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"layer_adaptation\": False,\n",
    "        },\n",
    "    ]\n",
    "    return param_groups\n",
    "\n",
    "\n",
    "def default(val, def_val):\n",
    "    return def_val if val is None else val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Augment:\n",
    "    \"\"\"\n",
    "    a probabilistic data augmentation module\n",
    "    Transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x_i and  x_j which we consider a positive pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, norm_means, norm_stds, s=1):\n",
    "        color_jitter = T.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "        # 10% of the image\n",
    "        blur = T.GaussianBlur(\n",
    "            kernel_size=(\n",
    "                3,\n",
    "                3,\n",
    "            ),\n",
    "            sigma=(0.1, 0.2),\n",
    "        )\n",
    "\n",
    "        self.train_transform = T.Compose(\n",
    "            [\n",
    "                # Crop image on a random scale from 7% tpo 100%\n",
    "                T.RandomResizedCrop(size=img_size),\n",
    "                # Flip image horizontally with 50% probability\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                # Apply heavy color jitter with 80% probability\n",
    "                T.RandomApply([color_jitter], p=0.8),\n",
    "                # Apply gaussian blur with 50% probability\n",
    "                T.RandomApply([blur], p=0.5),\n",
    "                # Convert RGB images to grayscale with 20% probability\n",
    "                T.RandomGrayscale(p=0.2),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(\n",
    "                    mean=norm_means,\n",
    "                    std=norm_stds,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AddProjection(nn.Module):\n",
    "    def __init__(self, config, model=None, mlp_dim=512):\n",
    "        super(AddProjection, self).__init__()\n",
    "        embedding_size = config.embedding_size\n",
    "        self.backbone = default(\n",
    "            model, models.resnet18(weights=None, num_classes=config.embedding_size)\n",
    "        )\n",
    "        mlp_dim = default(mlp_dim, self.backbone.fc.in_features)\n",
    "        print(\"DIM MLP input:\", mlp_dim)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # add mlp projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=mlp_dim, out_features=mlp_dim),\n",
    "            nn.BatchNorm1d(mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=mlp_dim, out_features=embedding_size),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        embedding = self.backbone(x)\n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        return self.projection(embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimCLRTraining(pl.LightningModule):\n",
    "    def __init__(self, config, norm_means, norm_stds, model=None, feat_dim=512):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.augment = Augment(\n",
    "            config.img_size, norm_means=norm_means, norm_stds=norm_stds\n",
    "        )\n",
    "        self.model = AddProjection(config, model=model, mlp_dim=feat_dim)\n",
    "\n",
    "        self.loss = InfoNceLoss(temperature=self.config.temperature)\n",
    "\n",
    "    def forward(self, batch, *args, **kwargs) -> torch.Tensor:\n",
    "        return self.model(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, *args, **kwargs) -> torch.Tensor:\n",
    "        (x1, x2), labels = batch\n",
    "        z1 = self.model(x1)\n",
    "        z2 = self.model(x2)\n",
    "        loss = self.loss(z1, z2)\n",
    "        self.log(\n",
    "            \"InfoNCE loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        wd = self.config.weight_decay\n",
    "        lr = self.config.lr\n",
    "        max_epochs = int(self.config.epochs)\n",
    "        param_groups = define_parameter_groups(\n",
    "            self.model, weight_decay=wd, optimizer_name=\"adam\"\n",
    "        )\n",
    "        optimizer = Adam(param_groups, lr=lr, weight_decay=wd)\n",
    "\n",
    "        print(\n",
    "            f\"Optimizer Adam, \"\n",
    "            f\"Learning Rate {lr}, \"\n",
    "            f\"Effective batch size {self.config.batch_size * self.config.gradient_accumulation_steps}\"\n",
    "        )\n",
    "\n",
    "        scheduler_warmup = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer, warmup_epochs=10, max_epochs=max_epochs, warmup_start_lr=0.0\n",
    "        )\n",
    "        return [optimizer], [scheduler_warmup]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class InfoNceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    InfoNCE loss as in SimCLR paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_similarity_batch(a, b):\n",
    "        representations = torch.cat([a, b], dim=0)\n",
    "        return F.cosine_similarity(\n",
    "            representations.unsqueeze(1), representations.unsqueeze(0), dim=2\n",
    "        )\n",
    "\n",
    "    def forward(self, proj_1, proj_2):\n",
    "        \"\"\"\n",
    "        proj_1 and proj_2 are batched embeddings [batch, embedding_dim]\n",
    "        where corresponding indices are pairs\n",
    "        z_i, z_j as in the SimCLR paper\n",
    "        \"\"\"\n",
    "        assert proj_1.shape == proj_2.shape, \"Projections' shapes need to match\"\n",
    "        batch_size = proj_1.shape[0]\n",
    "        mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=torch.bool)).float()\n",
    "\n",
    "        z_i = F.normalize(proj_1, p=2, dim=1)\n",
    "        z_j = F.normalize(proj_2, p=2, dim=1)\n",
    "\n",
    "        similarity_matrix = self.calc_similarity_batch(z_i, z_j)\n",
    "\n",
    "        sim_ij = torch.diag(similarity_matrix, batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -batch_size)\n",
    "\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "\n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "\n",
    "        print(\"mask\", device_as(mask, similarity_matrix).shape)\n",
    "        print(\"exp\", torch.exp(similarity_matrix / self.temperature).shape)\n",
    "        denominator = device_as(mask, similarity_matrix) * torch.exp(\n",
    "            similarity_matrix / self.temperature\n",
    "        )\n",
    "\n",
    "        all_losses = torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(all_losses) / (2 * batch_size)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Machine setup\n",
    "available_gpus = torch.cuda.device_count()\n",
    "save_model_path = os.path.join(os.getcwd(), \"saved_models/\")\n",
    "print(\"available_gpus:\", available_gpus)\n",
    "\n",
    "# Run setup\n",
    "filename = \"SimCLR_ResNet18_adam\"\n",
    "save_name = filename + \".ckpt\"\n",
    "resume_from_checkpoint = False\n",
    "\n",
    "\n",
    "# Model Setup\n",
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        self.epochs = 1  # number of training epochs\n",
    "        self.seed = 1234  # randomness seed\n",
    "        self.cuda = False  # use nvidia gpu\n",
    "        self.img_size = 64  # image shape\n",
    "        self.save = \"./saved_models/\"  # save checkpoint\n",
    "        self.gradient_accumulation_steps = 1  # gradient accumulation steps\n",
    "        self.batch_size = 64\n",
    "        self.lr = 1e-3\n",
    "        self.embedding_size = 128  # papers value is 128\n",
    "        self.temperature = 0.5  # 0.1 or 0.5\n",
    "        self.weight_decay = 1e-6\n",
    "\n",
    "\n",
    "train_config = Hparams()\n",
    "reproducibility(train_config)\n",
    "\n",
    "model = SimCLRTraining(\n",
    "    config=train_config,\n",
    "    model=models.resnet18(weights=None),\n",
    "    feat_dim=512,\n",
    "    norm_means=means,\n",
    "    norm_stds=stds,\n",
    ")\n",
    "\n",
    "transform = Augment(train_config.img_size, norm_means=means, norm_stds=stds)\n",
    "\n",
    "dataset = EuroSAT(\"./\", transform=transform, download=True)\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset, batch_size=train_config.batch_size, num_workers=cpu_count()\n",
    ")\n",
    "\n",
    "# Needed to get simulate a large batch size\n",
    "accumulator = GradientAccumulationScheduler(scheduling={0: 1})\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename=filename,\n",
    "    dirpath=save_model_path,\n",
    "    every_n_epochs=2,\n",
    "    save_last=True,\n",
    "    save_top_k=2,\n",
    "    monitor=\"InfoNCE loss_epoch\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    trainer = Trainer(\n",
    "        callbacks=[accumulator, checkpoint_callback],\n",
    "        gpus=available_gpus,\n",
    "        max_epochs=train_config.epochs,\n",
    "        resume_from_checkpoint=train_config.checkpoint_path,\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        callbacks=[accumulator, checkpoint_callback],\n",
    "        gpus=available_gpus,\n",
    "        max_epochs=train_config.epochs,\n",
    "    )\n",
    "\n",
    "trainer.fit(model, data_loader)\n",
    "trainer.save_checkpoint(save_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ssl-remote-sensing",
   "language": "python",
   "display_name": "ssl-remote-sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}