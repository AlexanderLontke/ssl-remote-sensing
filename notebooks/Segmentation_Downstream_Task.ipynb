{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlexanderLontke/ssl-remote-sensing/blob/vae-segmentation/notebooks/Segmentation_Downstream_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Segmentation downstream task: SEN12FLOOD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model: ResNetUnet \\\\\n",
    "Data: SEN12FLOOD \\\\\n",
    "Pretrained weights: dependent on pretext tasks\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Environment setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install ssl_remote_sensing@git+https://github.com/AlexanderLontke/ssl-remote-sensing.git@vae-segmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install rasterio torchmetrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # from ssl_remote_sensing.downstream_tasks.segmentatio\n",
    "from ssl_remote_sensing.data.dfc2020 import DFC2020\n",
    "from ssl_remote_sensing.downstream_tasks.segmentation.utils import (\n",
    "    patch_first_conv,\n",
    "    get_metrics,\n",
    ")\n",
    "from ssl_remote_sensing.downstream_tasks.segmentation.model import ResNetUNet\n",
    "from ssl_remote_sensing.pretext_tasks.vae.model import VariationalAutoencoder\n",
    "from ssl_remote_sensing.pretext_tasks.vae.utils import reproducibility\n",
    "from ssl_remote_sensing.constants import RANDOM_INITIALIZATION\n",
    "from ssl_remote_sensing.pretext_tasks.utils import (\n",
    "    load_encoder_checkpoint_from_pretext_model,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import JaccardIndex\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, jaccard_score\n",
    "import gdown\n",
    "import tarfile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\").type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfc2020_path = \"/content/drive/MyDrive/dfc2020/dfc_2020.tar.gz\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with tarfile.open(dfc2020_path, mode=\"r\") as tar:\n",
    "    tar.extractall(path=\"/content/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# subset - 'val' 986\n",
    "# subset - 'test' 5128\n",
    "\n",
    "# train_set for sentinel-2\n",
    "train_set = DFC2020(\n",
    "    \"/content/dfc_data\",\n",
    "    subset=\"test\",\n",
    "    use_s1=False,\n",
    "    use_s2lr=True,\n",
    "    use_s2hr=True,\n",
    "    use_s2mr=True,\n",
    "    no_savanna=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set.visualize_observation(170)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_set = DFC2020(\n",
    "    \"/content/dfc_data\",\n",
    "    subset=\"val\",\n",
    "    use_s1=False,\n",
    "    use_s2lr=True,\n",
    "    use_s2hr=True,\n",
    "    use_s2mr=True,\n",
    "    no_savanna=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_set.visualize_observation(170)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(val_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RANDOM_INITIALIZATION"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_drive_path = \"/content/drive/MyDrive/deep_learning_checkpoints\"\n",
    "check_point_paths = os.listdir(g_drive_path)\n",
    "check_point_paths += [RANDOM_INITIALIZATION]\n",
    "# check_point_paths.append(RANDOM_INITIALIZATION)\n",
    "check_point_paths = [g_drive_path + \"/\" + x for x in check_point_paths]\n",
    "check_point_paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for file in check_point_paths:\n",
    "#   if file == '/content/drive/MyDrive/deep_learning_checkpoints/random':\n",
    "#     print(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def display_outputs(idx=None, multi=False):\n",
    "#     # Pick a random index if none is specified\n",
    "#     if not idx:\n",
    "#         idx = random.randint(0, len(valset))\n",
    "#     print('Validation image ID: {}'.format(idx))\n",
    "\n",
    "#     # Get Sentinel 2 and Sentinel 1 data\n",
    "#     s2_data = torch.unsqueeze(valset.__getitem__(idx)['s2_img'].float().to(device), 0)\n",
    "#     s1_data = torch.unsqueeze(valset.__getitem__(idx)['s1_img'].float().to(device), 0)\n",
    "\n",
    "#     # Get predictions from the model\n",
    "#     if multi:\n",
    "#         output = model(s1_data, s2_data)\n",
    "#     else:\n",
    "#         output = model(s2_data)\n",
    "\n",
    "#     # Threshold the output to generate the binary map (FYI: the threshold value \"0\" can be tuned as any other hyperparameter)\n",
    "#     output_binary = torch.zeros(output.shape)\n",
    "#     output_binary[output >= 0] = 1\n",
    "\n",
    "#     get_metrics(valset.__getitem__(idx)['mask'], output_binary)\n",
    "\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(15, 7))\n",
    "#     axes[0].imshow(np.transpose(valset.__getitem__(idx)['s2_img'][[3,2,1],:,:], (1, 2, 0)) / valset.__getitem__(idx)['s2_img'].max())\n",
    "#     axes[0].set_title('True Color Sentinel-2')\n",
    "#     axes[2].imshow(valset.__getitem__(idx)['mask'], cmap='Blues')\n",
    "#     axes[2].set_title('Groundtruth')\n",
    "#     axes[1].imshow(output_binary.squeeze(), cmap='Blues')\n",
    "#     axes[1].set_title('Predicted Mask')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        # self.pretext_task = \"VAE\"\n",
    "        # self.pretext_task = \"SimCLR\"\n",
    "        # self.pretext_saved_name = 'autoencoder.ckpt'\n",
    "        self.checkpoint_name = None\n",
    "        # self.pretext_saved_name = 'SimCLR_ResNet18_adam-v3.ckpt'\n",
    "        self.epochs = 10  # number of training epochs for pretext tasks\n",
    "        self.seed = 1234  # randomness seed\n",
    "        self.save = \"./saved_model\"\n",
    "        self.gradient_accumulation_steps = 1  # gradient accumulation steps\n",
    "        self.batch_size = 16\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-6\n",
    "        self.latent_dim = 256\n",
    "        self.optim = \"Adam\"\n",
    "        self.embedding_size = 128  # papers value is 128\n",
    "        self.temperature = 0.5  # 0.1 or 0.5\n",
    "        self.weight_decay = 1e-6\n",
    "        self.cuda = True  # use coda\n",
    "        self.transform = False\n",
    "        self.split = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_config = Hparams()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reproducibility(train_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Directory & Wandb setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SEN12FLOOD \\\\\n",
    "\n",
    "\n",
    "*   13 Bands\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainset = SEN12FLOODS(root=\"/content/chips/\", transforms=True, split=\"train\")\n",
    "\n",
    "valset = SEN12FLOODS(root=\"/content/chips/\", split=\"val\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=8, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(valset, batch_size=8, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"[LOG] Shape of mask image is:\", next(iter(train_loader))[\"mask\"].shape)\n",
    "print(\"[LOG] Shape of sentinel-2 image is:\", next(iter(train_loader))[\"s2_img\"].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainset.visualize_observation(196)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valset.visualize_observation(127)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valset.visualize_observation(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valset.visualize_observation(37)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First of all, let's verify if a GPU is available on our compute machine. If not, the cpu will be used instead.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device used: {}\".format(device))\n",
    "# Define a learning rate\n",
    "learning_rate = train_config.lr\n",
    "# Initialise the loss function and move it to the GPU if available\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise the loss function and move it to the GPU if available\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next(iter(train_loader))[\"s2_img\"].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load_encoder_checkpoint_from_pretext_model(\n",
    "#         path_to_checkpoint='/content/drive/MyDrive/deep_learning_checkpoints/random'\n",
    "#     )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for file in check_point_paths:\n",
    "#     print(file)\n",
    "#     print(load_encoder_checkpoint_from_pretext_model(\n",
    "#         path_to_checkpoint=file\n",
    "#     ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for filename in check_point_paths:\n",
    "    # Update checkpoint name\n",
    "    train_config.checkpoint_name = filename\n",
    "    # Load Encoder from different pre-text architectures\n",
    "    encoder = load_encoder_checkpoint_from_pretext_model(\n",
    "        path_to_checkpoint=filename,\n",
    "    )\n",
    "    patch_first_conv(encoder, 13, default_in_channels=3)\n",
    "\n",
    "    model_name = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "    model_dir = \"/content/drive/My Drive/deep_learning_segmentation_checkpoints\"\n",
    "    model_path = os.path.join(model_dir, f\"segmentation_{model_name}.ckpt\")\n",
    "    # make sure the directory path exists\n",
    "    assert os.path.exists(model_dir)\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"ssl-remote-sensing-segmentation\",\n",
    "        name=model_name,\n",
    "        config=train_config.__dict__,\n",
    "    )\n",
    "\n",
    "    # Model setup\n",
    "    model = ResNetUNet(1, encoder=encoder)\n",
    "\n",
    "    # Initialise the optimizer\n",
    "    if train_config.optim == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif train_config.optim == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs = train_config.epochs\n",
    "\n",
    "    # Move the model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Create lists for logging losses and evalualtion metrics:\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    train_ious = []\n",
    "\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_ious = []\n",
    "\n",
    "    # IoU\n",
    "    jaccard = JaccardIndex(num_classes=2).to(device)\n",
    "\n",
    "    # For every epoch\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        progress = tqdm(\n",
    "            enumerate(train_loader), desc=\"Train Loss: \", total=len(train_loader)\n",
    "        )\n",
    "\n",
    "        # Specify you are in training mode\n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        epoch_train_ious = 0\n",
    "        epoch_val_ious = 0\n",
    "\n",
    "        epoch_train_accs = 0\n",
    "        epoch_val_accs = 0\n",
    "\n",
    "        for i, batch in progress:\n",
    "            # Transfer data to GPU if available\n",
    "            data = batch[\"s2_img\"].float().to(device)\n",
    "            label = batch[\"mask\"].float().to(device)\n",
    "\n",
    "            # Make a forward pass\n",
    "            output = model(data)\n",
    "            # print(output.shape)\n",
    "\n",
    "            # Derive binary segmentation map from prediction\n",
    "            output_binary = torch.zeros(output.shape)\n",
    "            output_binary[output >= 0] = 1\n",
    "\n",
    "            # Compute IoU\n",
    "            epoch_train_ious += jaccard(output_binary.to(device), label.int()) / len(\n",
    "                train_loader\n",
    "            )\n",
    "\n",
    "            # Compute pixel accuracies\n",
    "            epoch_train_accs += torch.sum(output_binary.to(device) == label.int()) / (\n",
    "                len(train_loader) * (256 * 256) * 100\n",
    "            )\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output, label.unsqueeze(1))\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss over the eopch\n",
    "            epoch_train_loss += loss / len(train_loader)\n",
    "\n",
    "            progress.set_description(\n",
    "                \"Epoch = {}, Train Loss: {:.4f}\".format(epoch + 1, epoch_train_loss)\n",
    "            )\n",
    "\n",
    "        progress = tqdm(\n",
    "            enumerate(val_loader),\n",
    "            desc=\"val Loss: \",\n",
    "            total=len(val_loader),\n",
    "            position=0,\n",
    "            leave=True,\n",
    "        )\n",
    "\n",
    "        # Specify you are in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Deactivate autograd engine (no backpropagation allowed)\n",
    "        with torch.no_grad():\n",
    "            epoch_val_loss = 0\n",
    "            for j, batch in progress:\n",
    "                # Transfer Data to GPU if available\n",
    "                data = batch[\"s2_img\"].float().to(device)\n",
    "                label = batch[\"mask\"].float().to(device)\n",
    "\n",
    "                # Make a forward pass\n",
    "                output = model(data)\n",
    "\n",
    "                # Derive binary segmentation map from prediction\n",
    "                output_binary = torch.zeros(output.shape)\n",
    "                output_binary[output >= 0] = 1\n",
    "\n",
    "                # Compute IoU\n",
    "                epoch_val_ious += jaccard(output_binary.to(device), label.int()) / len(\n",
    "                    val_loader\n",
    "                )\n",
    "\n",
    "                # Compute pixel accuracies\n",
    "                epoch_val_accs += torch.sum(output_binary.to(device) == label.int()) / (\n",
    "                    len(val_loader) * (256 * 256) * 100\n",
    "                )\n",
    "\n",
    "                # Compute the loss\n",
    "                val_loss = criterion(output, label.unsqueeze(1))\n",
    "\n",
    "                # Accumulate the loss over the epoch\n",
    "                epoch_val_loss += val_loss / len(val_loader)\n",
    "\n",
    "                progress.set_description(\n",
    "                    \"Validation Loss: {:.4f}\".format(epoch_val_loss)\n",
    "                )\n",
    "\n",
    "        if epoch == 0:\n",
    "            best_val_loss = epoch_val_loss\n",
    "        else:\n",
    "            if epoch_val_loss <= best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                # Save only the best model\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(\"Saving Model...\")\n",
    "\n",
    "        # save result to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_loss_segmentation\": epoch_train_loss,\n",
    "                \"val_loss_segmentation\": epoch_val_loss,\n",
    "                \"train_iou_segmentation\": epoch_train_ious,\n",
    "                \"val_iou_segmentation\": epoch_val_ious,\n",
    "                \"train_acc_segmentation\": epoch_train_accs,\n",
    "                \"val_acc_segmentation\": epoch_val_accs,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # print(\"Epoch = \", epoch+1)\n",
    "        # Save losses in list, so that we can visualise them later.\n",
    "        train_losses.append(epoch_train_loss.cpu().detach().numpy())\n",
    "        val_losses.append(epoch_val_loss.cpu().detach().numpy())\n",
    "\n",
    "        # Save IoUs in list, so that we can visualise them later.\n",
    "        train_ious.append(epoch_train_ious.cpu().detach().numpy())\n",
    "        val_ious.append(epoch_val_ious.cpu().detach().numpy())\n",
    "        print(f\"train_iou is {epoch_train_ious:.4f}, val_iou is {epoch_val_ious:.4f}\")\n",
    "\n",
    "        # Save accuracies in list, so that we can visualise them later.\n",
    "        train_accs.append(epoch_train_accs.cpu().detach().numpy())\n",
    "        val_accs.append(epoch_val_accs.cpu().detach().numpy())\n",
    "        print(f\"train_acc is {epoch_train_accs:.4f}, val_acc is {epoch_val_accs:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "    # Initialise a UNet() model\n",
    "    ResNetUNet(1, encoder=encoder)\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # Move to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    def display_outputs(idx=None, multi=False):\n",
    "        # Pick a random index if none is specified\n",
    "        if not idx:\n",
    "            idx = random.randint(0, len(valset))\n",
    "        print(\"Validation image ID: {}\".format(idx))\n",
    "\n",
    "        # Get Sentinel 2 and Sentinel 1 data\n",
    "        s2_data = torch.unsqueeze(\n",
    "            valset.__getitem__(idx)[\"s2_img\"].float().to(device), 0\n",
    "        )\n",
    "        s1_data = torch.unsqueeze(\n",
    "            valset.__getitem__(idx)[\"s1_img\"].float().to(device), 0\n",
    "        )\n",
    "\n",
    "        # Get predictions from the model\n",
    "        if multi:\n",
    "            output = model(s1_data, s2_data)\n",
    "        else:\n",
    "            output = model(s2_data)\n",
    "\n",
    "        # Threshold the output to generate the binary map (FYI: the threshold value \"0\" can be tuned as any other hyperparameter)\n",
    "        output_binary = torch.zeros(output.shape)\n",
    "        output_binary[output >= 0] = 1\n",
    "\n",
    "        get_metrics(valset.__getitem__(idx)[\"mask\"], output_binary)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 7))\n",
    "        axes[0].imshow(\n",
    "            np.transpose(valset.__getitem__(idx)[\"s2_img\"][[3, 2, 1], :, :], (1, 2, 0))\n",
    "            / valset.__getitem__(idx)[\"s2_img\"].max()\n",
    "        )\n",
    "        axes[0].set_title(\"True Color Sentinel-2\")\n",
    "        axes[2].imshow(valset.__getitem__(idx)[\"mask\"], cmap=\"Blues\")\n",
    "        axes[2].set_title(f\"Groundtruth\")\n",
    "        axes[1].imshow(output_binary.squeeze(), cmap=\"Blues\")\n",
    "        axes[1].set_title(f\"Predicted Mask-{model_name}\")\n",
    "\n",
    "    print(\"Sample image: \", model_name)\n",
    "    display_outputs(37)\n",
    "    display_outputs(127)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function copied from: https://github.com/qubvel/segmentation_models.pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMZSkbmMWSBI/ED2GwMsVuE",
   "include_colab_link": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "162b273aaad34bd38bf3c42db81e0487": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43e23e4dc35a43e69e128237282a6e4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a79663edf16452fa79e28c75d239458": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50ee20329c134f1d9f9f6d559be457b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bfca8ea6278243c59e7623546488d72b",
       "IPY_MODEL_e316f32be43c465fa314f2eb46ebb667"
      ],
      "layout": "IPY_MODEL_c606c8a3cfe9415fb655ae643eca2a4f"
     }
    },
    "553c6ea181594e709f8fd68a9b5d1233": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dfd51f065e14066a2482bd9a8c00492": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e7b8fdee12240698ffab1ffae21a6e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e7d1b03b26d43c8a479e182dd25a024": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8809377df363407fb8e9e85352bf8850": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_162b273aaad34bd38bf3c42db81e0487",
      "placeholder": "​",
      "style": "IPY_MODEL_abbb0aba9bc249f5882d92ebe7bc0dbd",
      "value": "0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "9c44e6c2d2204da7ab75b922f9fec6d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4956376e0454079946e651c77ab25af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43e23e4dc35a43e69e128237282a6e4c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a79663edf16452fa79e28c75d239458",
      "value": 1
     }
    },
    "abbb0aba9bc249f5882d92ebe7bc0dbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acc6e1e765cc4ea281a8dd6e74f6a100": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8809377df363407fb8e9e85352bf8850",
       "IPY_MODEL_a4956376e0454079946e651c77ab25af"
      ],
      "layout": "IPY_MODEL_553c6ea181594e709f8fd68a9b5d1233"
     }
    },
    "bfca8ea6278243c59e7623546488d72b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c44e6c2d2204da7ab75b922f9fec6d8",
      "placeholder": "​",
      "style": "IPY_MODEL_5e7d1b03b26d43c8a479e182dd25a024",
      "value": "0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "c606c8a3cfe9415fb655ae643eca2a4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e316f32be43c465fa314f2eb46ebb667": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dfd51f065e14066a2482bd9a8c00492",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e7b8fdee12240698ffab1ffae21a6e7",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}