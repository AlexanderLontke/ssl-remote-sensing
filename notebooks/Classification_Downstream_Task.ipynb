{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Downstream Task "
   ],
   "metadata": {
    "id": "jg21-fZ0lbYb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ssl_remote_sensing@git+https://github.com/AlexanderLontke/ssl-remote-sensing.git@feature/pipeline"
   ],
   "metadata": {
    "id": "hhzpyIoDlFIO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from ssl_remote_sensing.downstream_tasks.classification.model import (\n",
    "    DownstreamClassificationNet,\n",
    ")\n",
    "from ssl_remote_sensing.constants import RANDOM_INITIALIZATION\n",
    "from ssl_remote_sensing.pretext_tasks.utils import (\n",
    "    load_encoder_checkpoint_from_pretext_model,\n",
    ")\n",
    "from ssl_remote_sensing.data.get_eurosat import get_eurosat_normalizer, get_eurosat_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Loading ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RunConfig:\n",
    "    def __init__(self):\n",
    "        self.num_epochs = 10  # number of training epochs\n",
    "        self.seed = 1234  # randomness seed\n",
    "        self.save = \"./saved_models/\"  # save checkpoint\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 1e-3\n",
    "        self.embedding_size = 128  # papers value is 128\n",
    "        self.test_split_ratio = 0.2\n",
    "        self.checkpoint_name = None\n",
    "\n",
    "\n",
    "config = RunConfig()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup data loading\n",
    "eurosat_normalizer = get_eurosat_normalizer()\n",
    "train_dl, test_dl = get_eurosat_dataloader(\n",
    "    root=\"./\",\n",
    "    transform=eurosat_normalizer,\n",
    "    batchsize=config.batch_size,\n",
    "    numworkers=os.cpu_count(),\n",
    "    split=(config.test_split_ratio==0.2),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training ##\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First of all, let's verify if a GPU is available on our compute machine. If not, the cpu will be used instead.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device used: {}\".format(device))\n",
    "\n",
    "# define the optimization criterion / loss function\n",
    "loss_criterion = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Checkpoint Loading ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "g_drive_path = \"/content/drive/MyDrive/deep_learning_checkpoints\"\n",
    "check_point_paths = os.listdir(g_drive_path)\n",
    "check_point_paths += [RANDOM_INITIALIZATION]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for filename in check_point_paths:\n",
    "    # Update checkpoint name\n",
    "    config.checkpoint_name = filename\n",
    "    # Load Encoder from different pre-text architectures\n",
    "    encoder = load_encoder_checkpoint_from_pretext_model(\n",
    "        path_to_checkpoint=filename,\n",
    "    )\n",
    "    wandb.init(\n",
    "        project=\"ssl-remote-sensing-classification\",\n",
    "        name=filename,\n",
    "        config=config.__dict__,\n",
    "    )\n",
    "    # Model Setup\n",
    "    model = DownstreamClassificationNet(input_dim=512, encoder=encoder).to(device)\n",
    "    # define learning rate and optimization strategy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    model.train()\n",
    "    train_epoch_loss = np.NaN\n",
    "    validation_epoch_loss = np.NaN\n",
    "\n",
    "    with tqdm(range(config.num_epochs)) as tq:\n",
    "        for epoch in tq:\n",
    "            # print epoch loss\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            tq.desc = f\"[{now}] epoch: {epoch+1} train-loss: {train_epoch_loss} validation-loss: {validation_epoch_loss}\"\n",
    "            # init collection of mini-batch losses\n",
    "            train_mini_batch_losses = []\n",
    "\n",
    "            # iterate over all-mini batches\n",
    "            for i, (images, labels) in enumerate(train_dl):\n",
    "\n",
    "                # push mini-batch data to computation device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                optimizer.zero_grad()\n",
    "                out = model(images)\n",
    "                loss = loss_criterion(out, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # collect mini-batch reconstruction loss\n",
    "                train_mini_batch_losses.append(loss.data.item())\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"step/training_loss\": loss.data.item(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # determine mean min-batch loss of epoch\n",
    "            train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "\n",
    "            # Specify you are in evaluation mode\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                validation_mini_batch_losses = []\n",
    "                for (images, labels) in test_dl:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    # calculate outputs by running images through the network\n",
    "                    outputs = model(images)\n",
    "                    # the class with the highest energy is what we choose as prediction\n",
    "                    validation_epoch_loss = loss_criterion(outputs, labels)\n",
    "                    # collect mini-batch reconstruction loss\n",
    "                    validation_mini_batch_losses.append(\n",
    "                        validation_epoch_loss.data.item()\n",
    "                    )\n",
    "                validation_epoch_loss = np.mean(validation_mini_batch_losses)\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch/training_loss\": train_epoch_loss,\n",
    "                    \"epoch/validation_loss\": validation_epoch_loss,\n",
    "                }\n",
    "            )\n",
    "    # Store classification report\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    # iterate over test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in tqdm(test_dl, desc=\"Predict labels\"):\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images)  # Feed Network\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            y_pred.extend(predicted.cpu().numpy())  # Save Prediction\n",
    "            y_true.extend(labels.numpy())  # Save Truth\n",
    "    # log results\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"classification_report\": classification_report(\n",
    "                y_true, y_pred, output_dict=True\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    print(classification_report(y_true, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}